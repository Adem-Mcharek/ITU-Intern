{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITU WebTV Processing Pipeline - Standalone Notebook\n",
    "\n",
    "A simplified version of the complete processing pipeline that can run independently.\n",
    "Processes video/audio content into structured transcripts with AI-powered speaker identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install yt-dlp openai-whisper google-generativeai python-docx torch\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import timedelta, datetime\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Optional imports with graceful fallbacks\n",
    "try:\n",
    "    import yt_dlp\n",
    "    print(\"‚úÖ yt-dlp available\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå yt-dlp not available - video processing disabled\")\n",
    "    yt_dlp = None\n",
    "\n",
    "try:\n",
    "    import whisper\n",
    "    import torch\n",
    "    print(f\"‚úÖ Whisper available - GPU: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Whisper not available - transcription disabled\")\n",
    "    whisper = None\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    print(\"‚úÖ Google Gemini available\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Google Gemini not available - AI features disabled\")\n",
    "    genai = None\n",
    "\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.shared import Inches, Pt, RGBColor\n",
    "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "    print(\"‚úÖ python-docx available\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå python-docx not available - document generation disabled\")\n",
    "    Document = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "GEMINI_API_KEY = \"your-gemini-api-key-here\"  # Replace with your API key\n",
    "OUTPUT_DIR = \"output\"  # Directory for processed files\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite-preview-06-17\"\n",
    "WHISPER_MODEL = \"medium.en\"  # or \"base.en\" for faster processing\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Audio Download Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio_from_url(url, output_dir):\n",
    "    \"\"\"Download audio from video URL using yt-dlp\"\"\"\n",
    "    if not yt_dlp:\n",
    "        raise Exception(\"yt-dlp not available\")\n",
    "    \n",
    "    output_path = Path(output_dir) / 'audio.mp3'\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestaudio[ext=m4a]/bestaudio[ext=webm]/bestaudio/best',\n",
    "        'outtmpl': str(output_path.with_suffix('')),\n",
    "        'extractaudio': True,\n",
    "        'audioformat': 'mp3',\n",
    "        'audioquality': '192k',\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'no_warnings': False,\n",
    "        'quiet': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            print(f\"üì• Downloading audio from: {url}\")\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            title = info.get('title', 'Unknown')\n",
    "            duration = info.get('duration', 0)\n",
    "            \n",
    "            ydl.download([url])\n",
    "            \n",
    "            # Ensure MP3 file exists\n",
    "            if not output_path.exists():\n",
    "                for file in Path(output_dir).iterdir():\n",
    "                    if file.suffix.lower() in ['.mp3', '.m4a', '.wav']:\n",
    "                        file.rename(output_path)\n",
    "                        break\n",
    "            \n",
    "            if not output_path.exists():\n",
    "                raise FileNotFoundError(\"Audio file not found after download\")\n",
    "                \n",
    "            print(f\"‚úÖ Audio downloaded: {output_path}\")\n",
    "            return output_path, {'title': title, 'duration': duration}\n",
    "            \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to download audio: {str(e)}\")\n",
    "\n",
    "def use_local_audio_file(file_path, output_dir):\n",
    "    \"\"\"Copy local audio file to output directory\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Audio file not found: {file_path}\")\n",
    "    \n",
    "    output_path = Path(output_dir) / 'audio.mp3'\n",
    "    \n",
    "    # Copy or convert to MP3 if needed\n",
    "    if file_path.suffix.lower() == '.mp3':\n",
    "        import shutil\n",
    "        shutil.copy2(file_path, output_path)\n",
    "    else:\n",
    "        # Use FFmpeg to convert to MP3\n",
    "        try:\n",
    "            cmd = [\n",
    "                'ffmpeg', '-i', str(file_path),\n",
    "                '-acodec', 'mp3', '-ab', '192k',\n",
    "                '-y', str(output_path)\n",
    "            ]\n",
    "            subprocess.run(cmd, check=True, capture_output=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            # Fallback: just copy the file\n",
    "            import shutil\n",
    "            shutil.copy2(file_path, output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Audio file ready: {output_path}\")\n",
    "    return output_path, {'title': file_path.stem, 'duration': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Audio Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, output_dir, model_size=\"medium.en\"):\n",
    "    \"\"\"Transcribe audio using OpenAI Whisper\"\"\"\n",
    "    if not whisper:\n",
    "        raise Exception(\"Whisper not available\")\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"üé§ Loading Whisper model '{model_size}' on {device.upper()}\")\n",
    "    \n",
    "    model = whisper.load_model(model_size, device=device)\n",
    "    \n",
    "    print(f\"üéôÔ∏è Transcribing audio: {audio_path}\")\n",
    "    result = model.transcribe(str(audio_path), language=\"en\", verbose=False)\n",
    "    \n",
    "    # Save raw transcript\n",
    "    transcript_path = output_dir / 'transcript.txt'\n",
    "    with open(transcript_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(result['text'].strip())\n",
    "    \n",
    "    # Generate SRT file\n",
    "    srt_path = output_dir / 'transcript.srt'\n",
    "    with open(srt_path, 'w', encoding='utf-8') as f:\n",
    "        for i, segment in enumerate(result['segments'], 1):\n",
    "            start_time = format_srt_time(segment['start'])\n",
    "            end_time = format_srt_time(segment['end'])\n",
    "            text = segment['text'].strip()\n",
    "            \n",
    "            f.write(f\"{i}\\n\")\n",
    "            f.write(f\"{start_time} --> {end_time}\\n\")\n",
    "            f.write(f\"{text}\\n\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Transcription complete: {len(result['text'])} chars, {len(result['segments'])} segments\")\n",
    "    return transcript_path, srt_path, result['segments']\n",
    "\n",
    "def format_srt_time(seconds):\n",
    "    \"\"\"Convert seconds to SRT time format\"\"\"\n",
    "    td = timedelta(seconds=seconds)\n",
    "    hours, remainder = divmod(td.total_seconds(), 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds % 1) * 1000)\n",
    "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{milliseconds:03d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: SRT to JSON Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def srt_to_json(srt_path):\n",
    "    \"\"\"Convert SRT file to JSON format for AI processing\"\"\"\n",
    "    with open(srt_path, 'r', encoding='utf-8') as f:\n",
    "        srt_content = f.read()\n",
    "\n",
    "    cues = []\n",
    "    pattern = r'(\\d+)\\n([\\d:,]+) --> ([\\d:,]+)\\n(.*?)(?=\\n\\n\\d+\\n|$)'\n",
    "    \n",
    "    for match in re.finditer(pattern, srt_content, re.DOTALL):\n",
    "        index = int(match.group(1))\n",
    "        start = match.group(2).replace(',', '.')\n",
    "        end = match.group(3).replace(',', '.')\n",
    "        text = match.group(4).strip().replace('\\n', ' ')\n",
    "\n",
    "        cues.append({\n",
    "            \"index\": index,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"speaker\": \"\",\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    print(f\"üìù Converted SRT to JSON: {len(cues)} segments\")\n",
    "    return cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: AI Speaker Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_gemini_api():\n",
    "    \"\"\"Initialize Gemini API\"\"\"\n",
    "    if not genai or not GEMINI_API_KEY or GEMINI_API_KEY == \"your-gemini-api-key-here\":\n",
    "        return None\n",
    "    \n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    return genai.GenerativeModel(MODEL_NAME)\n",
    "\n",
    "def extract_speaker_context(transcript_text):\n",
    "    \"\"\"Extract speaker information from transcript using Gemini\"\"\"\n",
    "    model = setup_gemini_api()\n",
    "    if not model:\n",
    "        print(\"‚ö†Ô∏è Gemini API not available, skipping speaker context extraction\")\n",
    "        return {\"speakers\": []}\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert in transcript analysis and speaker identification.\n",
    "\n",
    "Analyze this transcript and extract information about all speakers mentioned.\n",
    "\n",
    "Identify:\n",
    "1. Speaker names (when they introduce themselves or are introduced)\n",
    "2. Their positions/titles\n",
    "3. Organizations they represent\n",
    "4. Countries they represent (if applicable)\n",
    "\n",
    "Return as JSON:\n",
    "{{\n",
    "    \"speakers\": [\n",
    "        {{\n",
    "            \"name\": \"Speaker Name\",\n",
    "            \"title\": \"Their title/position\",\n",
    "            \"organization\": \"Organization\",\n",
    "            \"country\": \"Country (if applicable)\",\n",
    "            \"description\": \"Brief description\"\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Transcript:\n",
    "{transcript_text}\n",
    "\n",
    "Return ONLY the JSON object:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üß† Extracting speaker context with Gemini AI...\")\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Clean response\n",
    "        cleaned = response.text.strip()\n",
    "        if cleaned.startswith(\"```json\"):\n",
    "            cleaned = cleaned[7:]\n",
    "        if cleaned.endswith(\"```\"):\n",
    "            cleaned = cleaned[:-3]\n",
    "        \n",
    "        speaker_info = json.loads(cleaned.strip())\n",
    "        \n",
    "        print(f\"‚úÖ Found {len(speaker_info.get('speakers', []))} speakers in context\")\n",
    "        return speaker_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting speaker context: {e}\")\n",
    "        return {\"speakers\": []}\n",
    "\n",
    "def create_speaker_context_prompt(speaker_info):\n",
    "    \"\"\"Create context prompt for speaker diarization\"\"\"\n",
    "    if not speaker_info.get('speakers'):\n",
    "        return \"\"\n",
    "    \n",
    "    context = \"\\n\\nKNOWN SPEAKERS IN THIS TRANSCRIPT:\\n\" + \"=\" * 50 + \"\\n\"\n",
    "    \n",
    "    for speaker in speaker_info['speakers']:\n",
    "        name = speaker.get('name', 'Unknown')\n",
    "        title = speaker.get('title', '')\n",
    "        org = speaker.get('organization', '')\n",
    "        country = speaker.get('country', '')\n",
    "        \n",
    "        context += f\"‚Ä¢ {name}\"\n",
    "        if title:\n",
    "            context += f\" - {title}\"\n",
    "        if org:\n",
    "            context += f\" at {org}\"\n",
    "        if country:\n",
    "            context += f\" (representing {country})\"\n",
    "        context += \"\\n\"\n",
    "    \n",
    "    context += \"=\" * 50 + \"\\n\"\n",
    "    context += \"Use these EXACT speaker names when you recognize them.\\n\\n\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Speaker Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_speakers_in_transcript(transcript_data, speaker_context=\"\"):\n",
    "    \"\"\"Use Gemini AI to identify speakers in transcript segments\"\"\"\n",
    "    model = setup_gemini_api()\n",
    "    if not model:\n",
    "        print(\"‚ö†Ô∏è Gemini API not available, returning transcript without speaker identification\")\n",
    "        return transcript_data\n",
    "    \n",
    "    # Process in batches to handle token limits\n",
    "    batch_size = 50  # Adjust based on token limits\n",
    "    filled_segments = []\n",
    "    \n",
    "    for i in range(0, len(transcript_data), batch_size):\n",
    "        batch = transcript_data[i:i + batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        total_batches = math.ceil(len(transcript_data) / batch_size)\n",
    "        \n",
    "        print(f\"üß† Processing speaker identification batch {batch_num}/{total_batches}...\")\n",
    "        \n",
    "        batch_string = json.dumps(batch, indent=2)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert in transcript analysis and speaker diarization.\n",
    "Analyze this transcript batch and identify who is speaking in each segment.\n",
    "\n",
    "{speaker_context}\n",
    "\n",
    "Instructions:\n",
    "1. Use the EXACT speaker names from the known speakers list when you recognize them\n",
    "2. For unknown speakers, use descriptive labels like 'Moderator', 'Participant 1', etc.\n",
    "3. Base identification on speech patterns, content, and context clues\n",
    "\n",
    "Return the complete JSON with \"speaker\" field filled for every segment:\n",
    "\n",
    "{batch_string}\n",
    "\n",
    "Return ONLY the filled JSON array:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            \n",
    "            # Clean response\n",
    "            cleaned = response.text.strip()\n",
    "            if cleaned.startswith(\"```json\"):\n",
    "                cleaned = cleaned[7:]\n",
    "            if cleaned.endswith(\"```\"):\n",
    "                cleaned = cleaned[:-3]\n",
    "            \n",
    "            filled_batch = json.loads(cleaned.strip())\n",
    "            filled_segments.extend(filled_batch)\n",
    "            \n",
    "            # Add delay to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing batch {batch_num}: {e}\")\n",
    "            # Use original batch if AI processing fails\n",
    "            filled_segments.extend(batch)\n",
    "    \n",
    "    print(f\"‚úÖ Speaker identification complete: {len(filled_segments)} segments\")\n",
    "    return filled_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Speaker-Separated Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_speaker_info(speaker_name):\n",
    "    \"\"\"Parse speaker name to extract name and organization\"\"\"\n",
    "    if not speaker_name or speaker_name.strip() == \"\":\n",
    "        return \"Unknown Speaker\", \"Unknown\"\n",
    "    \n",
    "    speaker_name = speaker_name.strip()\n",
    "    \n",
    "    # Pattern: \"Name (Organization)\"\n",
    "    paren_match = re.match(r'^(.+?)\\s*\\((.+?)\\)$', speaker_name)\n",
    "    if paren_match:\n",
    "        return paren_match.group(1).strip(), paren_match.group(2).strip()\n",
    "    \n",
    "    # Pattern: \"Name - Organization\"\n",
    "    dash_match = re.match(r'^(.+?)\\s*[‚Äì-]\\s*(.+)$', speaker_name)\n",
    "    if dash_match:\n",
    "        return dash_match.group(1).strip(), dash_match.group(2).strip()\n",
    "    \n",
    "    # Pattern: \"Name, Title, Organization\"\n",
    "    comma_parts = speaker_name.split(',')\n",
    "    if len(comma_parts) >= 2:\n",
    "        return comma_parts[0].strip(), ', '.join(comma_parts[1:]).strip()\n",
    "    \n",
    "    return speaker_name, \"Not specified\"\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    \"\"\"Convert HH:MM:SS.mmm to seconds\"\"\"\n",
    "    try:\n",
    "        if ':' in time_str:\n",
    "            parts = time_str.split(':')\n",
    "            if len(parts) == 3:\n",
    "                hours = float(parts[0])\n",
    "                minutes = float(parts[1])\n",
    "                seconds = float(parts[2])\n",
    "                return hours * 3600 + minutes * 60 + seconds\n",
    "        return float(time_str)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def group_consecutive_segments(transcript_data):\n",
    "    \"\"\"Group consecutive segments from the same speaker\"\"\"\n",
    "    if not transcript_data:\n",
    "        return []\n",
    "    \n",
    "    grouped_segments = []\n",
    "    current_group = {\n",
    "        'speaker': transcript_data[0].get('speaker', 'Unknown'),\n",
    "        'text_parts': [transcript_data[0].get('text', '')],\n",
    "        'start_time': time_to_seconds(transcript_data[0].get('start', '0')),\n",
    "        'end_time': time_to_seconds(transcript_data[0].get('end', '0')),\n",
    "    }\n",
    "    \n",
    "    for i in range(1, len(transcript_data)):\n",
    "        segment = transcript_data[i]\n",
    "        current_speaker = segment.get('speaker', 'Unknown')\n",
    "        \n",
    "        if current_speaker == current_group['speaker']:\n",
    "            # Same speaker, add to current group\n",
    "            current_group['text_parts'].append(segment.get('text', ''))\n",
    "            current_group['end_time'] = time_to_seconds(segment.get('end', current_group['end_time']))\n",
    "        else:\n",
    "            # Different speaker, save current group and start new one\n",
    "            current_group['combined_text'] = ' '.join(current_group['text_parts'])\n",
    "            grouped_segments.append(current_group.copy())\n",
    "            \n",
    "            current_group = {\n",
    "                'speaker': current_speaker,\n",
    "                'text_parts': [segment.get('text', '')],\n",
    "                'start_time': time_to_seconds(segment.get('start', '0')),\n",
    "                'end_time': time_to_seconds(segment.get('end', '0')),\n",
    "            }\n",
    "    \n",
    "    # Don't forget the last group\n",
    "    current_group['combined_text'] = ' '.join(current_group['text_parts'])\n",
    "    grouped_segments.append(current_group)\n",
    "    \n",
    "    return grouped_segments\n",
    "\n",
    "def create_speaker_separated_transcript(filled_transcript, output_dir, title=\"Meeting\"):\n",
    "    \"\"\"Create human-readable speaker-separated transcript\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Group consecutive segments\n",
    "    grouped = group_consecutive_segments(filled_transcript)\n",
    "    \n",
    "    # Create speaker transcript\n",
    "    speakers_path = output_dir / 'transcript_speakers.txt'\n",
    "    with open(speakers_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Speaker-separated transcript: {title}\\n\\n\")\n",
    "        \n",
    "        for segment in grouped:\n",
    "            speaker_name = segment['speaker']\n",
    "            clean_speaker, representing = parse_speaker_info(speaker_name)\n",
    "            content = segment['combined_text']\n",
    "            start_time = segment['start_time']\n",
    "            end_time = segment['end_time']\n",
    "            \n",
    "            # Format speaker header\n",
    "            if representing and representing != \"Not specified\":\n",
    "                speaker_header = f\"[{clean_speaker}, {representing}]\"\n",
    "            else:\n",
    "                speaker_header = f\"[{clean_speaker}]\"\n",
    "            \n",
    "            # Add timing\n",
    "            start_min = int(start_time // 60)\n",
    "            start_sec = int(start_time % 60)\n",
    "            end_min = int(end_time // 60)\n",
    "            end_sec = int(end_time % 60)\n",
    "            timing_info = f\" ({start_min:02d}:{start_sec:02d} - {end_min:02d}:{end_sec:02d})\"\n",
    "            speaker_header += timing_info\n",
    "            \n",
    "            f.write(f\"{speaker_header}\\n\")\n",
    "            f.write(f\"{content}\\n\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Speaker-separated transcript created: {speakers_path}\")\n",
    "    return speakers_path, grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate ITU Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_itu_summary(transcript_content):\n",
    "    \"\"\"Generate ITU-focused summary using Gemini\"\"\"\n",
    "    model = setup_gemini_api()\n",
    "    if not model:\n",
    "        print(\"‚ö†Ô∏è Gemini API not available, skipping ITU summary\")\n",
    "        return None\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an ITU staff member writing a brief internal summary for colleagues.\n",
    "\n",
    "Analyze this meeting transcript and write a concise summary focusing ONLY on what matters to ITU's work.\n",
    "\n",
    "ITU FOCUS AREAS (prioritize what's most relevant):\n",
    "‚Ä¢ Standards & Technical work (ITU-T, ITU-R)\n",
    "‚Ä¢ Digital inclusion & development (ITU-D)\n",
    "‚Ä¢ Emerging tech (AI, 5G/6G, IoT)\n",
    "‚Ä¢ Cybersecurity & trust\n",
    "‚Ä¢ Spectrum management\n",
    "‚Ä¢ Digital transformation initiatives\n",
    "‚Ä¢ ICT capacity building\n",
    "\n",
    "FORMAT:\n",
    "**Key ITU-Relevant Points:**\n",
    "‚Ä¢ [Most important point for ITU]\n",
    "‚Ä¢ [Second priority point]\n",
    "\n",
    "**Potential ITU Actions/Opportunities:**\n",
    "‚Ä¢ [What ITU could/should do based on this meeting]\n",
    "\n",
    "Maximum 150 words total. If minimal ICT content, write \"Limited relevance to ITU mandate - primarily [topic]\"\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript_content}\n",
    "\n",
    "Provide your ITU-focused summary:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üè¢ Generating ITU-focused summary...\")\n",
    "        response = model.generate_content(prompt)\n",
    "        summary = response.text.strip()\n",
    "        print(f\"‚úÖ ITU summary generated ({len(summary)} characters)\")\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating ITU summary: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Professional Meeting Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meeting_notes(transcript_content):\n",
    "    \"\"\"Generate professional meeting notes using Gemini\"\"\"\n",
    "    model = setup_gemini_api()\n",
    "    if not model:\n",
    "        print(\"‚ö†Ô∏è Gemini API not available, skipping meeting notes\")\n",
    "        return None\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Create professional meeting notes in ITU/UN diplomatic style.\n",
    "\n",
    "Use this EXACT structure:\n",
    "\n",
    "**MEETING OVERVIEW**\n",
    "Brief purpose, key participants, main themes (2-3 sentences only)\n",
    "\n",
    "**KEY DISCUSSIONS**\n",
    "Main topics with speaker attribution. Format: \"[Speaker Name, Organization] emphasized that...\"\n",
    "\n",
    "**POSITIONS & RECOMMENDATIONS**\n",
    "Member state positions and organizational viewpoints\n",
    "\n",
    "**DECISIONS & ACTION ITEMS**\n",
    "‚Ä¢ Specific decisions made\n",
    "‚Ä¢ Action items with responsible parties\n",
    "‚Ä¢ Timelines and next steps\n",
    "\n",
    "**TECHNICAL MATTERS** (only if significant technical content)\n",
    "Standards, specifications, implementation issues\n",
    "\n",
    "STYLE: Formal UN/ITU language, third person, speaker attribution, concise sections.\n",
    "\n",
    "TRANSCRIPT:\n",
    "{transcript_content}\n",
    "\n",
    "Generate professional meeting notes:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üìù Generating professional meeting notes...\")\n",
    "        response = model.generate_content(prompt)\n",
    "        notes = response.text.strip()\n",
    "        print(f\"‚úÖ Meeting notes generated ({len(notes)} characters)\")\n",
    "        return notes\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating meeting notes: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_word_document(notes_content, title, output_dir):\n",
    "    \"\"\"Create formatted Word document\"\"\"\n",
    "    if not Document:\n",
    "        print(\"‚ö†Ô∏è python-docx not available, saving as text file\")\n",
    "        text_path = Path(output_dir) / f\"meeting_notes_{datetime.now().strftime('%Y%m%d')}.txt\"\n",
    "        with open(text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"MEETING NOTES\\n{'='*50}\\n\\n\")\n",
    "            f.write(f\"Title: {title}\\n\")\n",
    "            f.write(f\"Date: {datetime.now().strftime('%B %d, %Y')}\\n\\n\")\n",
    "            f.write(notes_content)\n",
    "        return text_path\n",
    "    \n",
    "    # Create Word document\n",
    "    doc = Document()\n",
    "    \n",
    "    # Set margins\n",
    "    sections = doc.sections\n",
    "    for section in sections:\n",
    "        section.top_margin = Inches(1)\n",
    "        section.bottom_margin = Inches(1)\n",
    "        section.left_margin = Inches(1)\n",
    "        section.right_margin = Inches(1)\n",
    "    \n",
    "    # Add header\n",
    "    header = doc.sections[0].header\n",
    "    header_para = header.paragraphs[0]\n",
    "    header_para.text = \"International Telecommunication Union\"\n",
    "    header_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Document title\n",
    "    title_heading = doc.add_heading('MEETING NOTES', 0)\n",
    "    title_heading.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Meeting title\n",
    "    meeting_title = doc.add_heading(title, 1)\n",
    "    meeting_title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Date\n",
    "    date_para = doc.add_paragraph()\n",
    "    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    date_run = date_para.add_run(f\"Date: {datetime.now().strftime('%B %d, %Y')}\")\n",
    "    date_run.italic = True\n",
    "    \n",
    "    # Separator\n",
    "    doc.add_paragraph(\"_\" * 80).alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    doc.add_paragraph()\n",
    "    \n",
    "    # Add content\n",
    "    lines = notes_content.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Section headers\n",
    "        if line.startswith('**') and line.endswith('**'):\n",
    "            section_title = line[2:-2].strip()\n",
    "            doc.add_heading(section_title, 2)\n",
    "        # Bullet points\n",
    "        elif line.startswith('‚Ä¢') or line.startswith('-'):\n",
    "            doc.add_paragraph(line[1:].strip(), style='List Bullet')\n",
    "        # Regular paragraphs\n",
    "        else:\n",
    "            doc.add_paragraph(line)\n",
    "    \n",
    "    # Footer\n",
    "    footer = doc.sections[0].footer\n",
    "    footer_para = footer.paragraphs[0]\n",
    "    footer_para.text = f\"Generated on {datetime.now().strftime('%B %d, %Y at %H:%M UTC')} | ITU Processing System\"\n",
    "    footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "    \n",
    "    # Save document\n",
    "    safe_title = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "    safe_title = safe_title.replace(' ', '_')[:30]\n",
    "    doc_path = Path(output_dir) / f\"Meeting_Notes_{safe_title}_{datetime.now().strftime('%Y%m%d')}.docx\"\n",
    "    doc.save(str(doc_path))\n",
    "    \n",
    "    print(f\"‚úÖ Word document created: {doc_path}\")\n",
    "    return doc_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meeting(input_source, title=\"Meeting\", is_url=True):\n",
    "    \"\"\"Complete processing pipeline\"\"\"\n",
    "    print(f\"üöÄ Starting processing pipeline for: {title}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create session directory\n",
    "    session_dir = Path(OUTPUT_DIR) / f\"meeting_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    session_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Get audio\n",
    "        if is_url:\n",
    "            audio_path, metadata = download_audio_from_url(input_source, session_dir)\n",
    "            if not title or title == \"Meeting\":\n",
    "                title = metadata.get('title', 'Meeting')\n",
    "        else:\n",
    "            audio_path, metadata = use_local_audio_file(input_source, session_dir)\n",
    "        \n",
    "        # Step 2: Transcribe\n",
    "        transcript_path, srt_path, segments = transcribe_audio(audio_path, session_dir, WHISPER_MODEL)\n",
    "        \n",
    "        # Step 3: Convert to JSON for AI processing\n",
    "        json_segments = srt_to_json(srt_path)\n",
    "        \n",
    "        # Step 4: Extract speaker context\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "            full_transcript = f.read()\n",
    "        \n",
    "        speaker_info = extract_speaker_context(full_transcript)\n",
    "        speaker_context = create_speaker_context_prompt(speaker_info)\n",
    "        \n",
    "        # Step 5: Fill speakers using AI\n",
    "        filled_transcript = fill_speakers_in_transcript(json_segments, speaker_context)\n",
    "        \n",
    "        # Step 6: Create speaker-separated transcript\n",
    "        speakers_path, structured_segments = create_speaker_separated_transcript(\n",
    "            filled_transcript, session_dir, title\n",
    "        )\n",
    "        \n",
    "        # Step 7: Generate ITU summary\n",
    "        itu_summary = generate_itu_summary(full_transcript)\n",
    "        if itu_summary:\n",
    "            summary_path = session_dir / 'itu_summary.txt'\n",
    "            with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(itu_summary)\n",
    "        \n",
    "        # Step 8: Generate meeting notes\n",
    "        meeting_notes = generate_meeting_notes(full_transcript)\n",
    "        if meeting_notes:\n",
    "            # Save as text\n",
    "            notes_text_path = session_dir / 'meeting_notes.txt'\n",
    "            with open(notes_text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(meeting_notes)\n",
    "            \n",
    "            # Save as Word document\n",
    "            doc_path = create_word_document(meeting_notes, title, session_dir)\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"‚úÖ PROCESSING COMPLETE!\")\n",
    "        print(f\"üìÅ Output directory: {session_dir}\")\n",
    "        print(f\"üéµ Audio: {audio_path.name}\")\n",
    "        print(f\"üìù Transcript: {transcript_path.name}\")\n",
    "        print(f\"üé¨ Subtitles: {srt_path.name}\")\n",
    "        print(f\"üë• Speaker transcript: {speakers_path.name}\")\n",
    "        if itu_summary:\n",
    "            print(f\"üè¢ ITU summary: itu_summary.txt\")\n",
    "        if meeting_notes:\n",
    "            print(f\"üìÑ Meeting notes: meeting_notes.txt\")\n",
    "            if Document:\n",
    "                print(f\"üìé Word document: {doc_path.name}\")\n",
    "        \n",
    "        return {\n",
    "            'session_dir': session_dir,\n",
    "            'audio_path': audio_path,\n",
    "            'transcript_path': transcript_path,\n",
    "            'srt_path': srt_path,\n",
    "            'speakers_path': speakers_path,\n",
    "            'segments': structured_segments,\n",
    "            'itu_summary': itu_summary,\n",
    "            'meeting_notes': meeting_notes\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Processing failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Run the cells below to process your content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Process a YouTube video\n",
    "# Replace with your video URL\n",
    "video_url = \"https://www.youtube.com/watch?v=your-video-id\"\n",
    "# result = process_meeting(video_url, \"Sample YouTube Video\", is_url=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Process a local audio file\n",
    "# Replace with your audio file path\n",
    "audio_file = \"path/to/your/audio/file.mp3\"\n",
    "# result = process_meeting(audio_file, \"Local Audio File\", is_url=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Process UN WebTV content\n",
    "# Replace with actual UN WebTV URL\n",
    "webtv_url = \"https://webtv.un.org/asset/your-asset-id\"\n",
    "# result = process_meeting(webtv_url, \"UN WebTV Meeting\", is_url=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Processing Function\n",
    "\n",
    "For quick processing without all the setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_process(input_source, title=None):\n",
    "    \"\"\"Quick processing function - detects if input is URL or file\"\"\"\n",
    "    \n",
    "    # Auto-detect if input is URL or file\n",
    "    is_url = input_source.startswith(('http://', 'https://'))\n",
    "    \n",
    "    if not title:\n",
    "        if is_url:\n",
    "            title = \"Video Processing\"\n",
    "        else:\n",
    "            title = Path(input_source).stem\n",
    "    \n",
    "    return process_meeting(input_source, title, is_url)\n",
    "\n",
    "# Usage:\n",
    "# result = quick_process(\"https://youtube.com/watch?v=abc123\")\n",
    "# result = quick_process(\"/path/to/audio.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
